"""
streams_provider

Autogenerated DPF operator classes.
"""

from __future__ import annotations
from typing import TYPE_CHECKING

from warnings import warn
from ansys.dpf.core.dpf_operator import Operator
from ansys.dpf.core.inputs import Input, _Inputs
from ansys.dpf.core.outputs import Output, _Outputs
from ansys.dpf.core.operators.specification import PinSpecification, Specification
from ansys.dpf.core.config import Config
from ansys.dpf.core.server_types import AnyServerType

if TYPE_CHECKING:
    from ansys.dpf.core.data_sources import DataSources
    from ansys.dpf.core.streams_container import StreamsContainer


class streams_provider(Operator):
    r"""Create streams (cached file handles) based on provided data sources.

    - When the data sources contain a single result key, an internal
      provider is instantiated for that namespace to create streams.
    - When multiple result keys are present, streams are aggregated from
      each namespace-specific internal provider.
    - Upstream data sources (if defined) are opened and attached to the
      output as upstream streams.

    When the ``permissive`` configuration is enabled, the operator silently
    skips result files that cannot be opened or have unsupported namespaces,
    continuing with valid files. If ``permissive`` is disabled (default),
    errors are thrown for invalid files.

    Throws a runtime error if data sources are missing or if all result
    files are invalid.


    Inputs
    ------
    data_sources: DataSources
        Data sources describing one or more result files. Supports single or multiple result keys; may include upstream data sources.

        **Error conditions when `permissive` config is disabled:**
        - Throws `std::runtime_error` with message "StreamProvider: empty namespace for result key '<key>'" if a result file has no namespace.
        - Throws `std::runtime_error` with message "StreamProvider: operator <namespace>::stream_provider not found." if the namespace is not supported.
        - If all result files are invalid, throws an error even in permissive mode.

    Outputs
    -------
    streams_container: StreamsContainer
        Streams created from the input data sources, including aggregated streams across namespaces and any upstream streams.

    Examples
    --------
    >>> from ansys.dpf import core as dpf

    >>> # Instantiate operator
    >>> op = dpf.operators.metadata.streams_provider()

    >>> # Make input connections
    >>> my_data_sources = dpf.DataSources()
    >>> op.inputs.data_sources.connect(my_data_sources)

    >>> # Instantiate operator and connect inputs in one line
    >>> op = dpf.operators.metadata.streams_provider(
    ...     data_sources=my_data_sources,
    ... )

    >>> # Get output data
    >>> result_streams_container = op.outputs.streams_container()
    """

    def __init__(self, data_sources=None, config=None, server=None):
        super().__init__(
            name="stream_provider",
            config=config,
            server=server,
            inputs_type=InputsStreamsProvider,
            outputs_type=OutputsStreamsProvider,
        )
        if data_sources is not None:
            self.inputs.data_sources.connect(data_sources)

    @staticmethod
    def _spec() -> Specification:
        description = r"""Create streams (cached file handles) based on provided data sources.

- When the data sources contain a single result key, an internal
  provider is instantiated for that namespace to create streams.
- When multiple result keys are present, streams are aggregated from
  each namespace-specific internal provider.
- Upstream data sources (if defined) are opened and attached to the
  output as upstream streams.

When the ``permissive`` configuration is enabled, the operator silently
skips result files that cannot be opened or have unsupported namespaces,
continuing with valid files. If ``permissive`` is disabled (default),
errors are thrown for invalid files.

Throws a runtime error if data sources are missing or if all result
files are invalid.
"""
        spec = Specification(
            description=description,
            map_input_pin_spec={
                4: PinSpecification(
                    name="data_sources",
                    type_names=["data_sources"],
                    optional=False,
                    document=r"""Data sources describing one or more result files. Supports single or multiple result keys; may include upstream data sources.

**Error conditions when `permissive` config is disabled:**
- Throws `std::runtime_error` with message "StreamProvider: empty namespace for result key '<key>'" if a result file has no namespace.
- Throws `std::runtime_error` with message "StreamProvider: operator <namespace>::stream_provider not found." if the namespace is not supported.
- If all result files are invalid, throws an error even in permissive mode.""",
                ),
            },
            map_output_pin_spec={
                0: PinSpecification(
                    name="streams_container",
                    type_names=["streams_container"],
                    optional=False,
                    document=r"""Streams created from the input data sources, including aggregated streams across namespaces and any upstream streams.""",
                ),
            },
        )
        return spec

    @staticmethod
    def default_config(server: AnyServerType = None) -> Config:
        """Returns the default config of the operator.

        This config can then be changed to the user needs and be used to
        instantiate the operator. The Configuration allows to customize
        how the operation will be processed by the operator.

        Parameters
        ----------
        server:
            Server with channel connected to the remote or local instance. When
            ``None``, attempts to use the global server.

        Returns
        -------
        config:
            A new Config instance equivalent to the default config for this operator.
        """
        return Operator.default_config(name="stream_provider", server=server)

    @property
    def inputs(self) -> InputsStreamsProvider:
        """Enables to connect inputs to the operator

        Returns
        --------
        inputs:
            An instance of InputsStreamsProvider.
        """
        return self._inputs

    @property
    def outputs(self) -> OutputsStreamsProvider:
        """Enables to get outputs of the operator by evaluating it

        Returns
        --------
        outputs:
            An instance of OutputsStreamsProvider.
        """
        return self._outputs


class InputsStreamsProvider(_Inputs):
    """Intermediate class used to connect user inputs to
    streams_provider operator.

    Examples
    --------
    >>> from ansys.dpf import core as dpf
    >>> op = dpf.operators.metadata.streams_provider()
    >>> my_data_sources = dpf.DataSources()
    >>> op.inputs.data_sources.connect(my_data_sources)
    """

    def __init__(self, op: Operator):
        super().__init__(streams_provider._spec().inputs, op)
        self._data_sources: Input[DataSources] = Input(
            streams_provider._spec().input_pin(4), 4, op, -1
        )
        self._inputs.append(self._data_sources)

    @property
    def data_sources(self) -> Input[DataSources]:
        r"""Allows to connect data_sources input to the operator.

        Data sources describing one or more result files. Supports single or multiple result keys; may include upstream data sources.

        **Error conditions when `permissive` config is disabled:**
        - Throws `std::runtime_error` with message "StreamProvider: empty namespace for result key '<key>'" if a result file has no namespace.
        - Throws `std::runtime_error` with message "StreamProvider: operator <namespace>::stream_provider not found." if the namespace is not supported.
        - If all result files are invalid, throws an error even in permissive mode.

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.metadata.streams_provider()
        >>> op.inputs.data_sources.connect(my_data_sources)
        >>> # or
        >>> op.inputs.data_sources(my_data_sources)
        """
        return self._data_sources


class OutputsStreamsProvider(_Outputs):
    """Intermediate class used to get outputs from
    streams_provider operator.

    Examples
    --------
    >>> from ansys.dpf import core as dpf
    >>> op = dpf.operators.metadata.streams_provider()
    >>> # Connect inputs : op.inputs. ...
    >>> result_streams_container = op.outputs.streams_container()
    """

    def __init__(self, op: Operator):
        super().__init__(streams_provider._spec().outputs, op)
        self._streams_container: Output[StreamsContainer] = Output(
            streams_provider._spec().output_pin(0), 0, op
        )
        self._outputs.append(self._streams_container)

    @property
    def streams_container(self) -> Output[StreamsContainer]:
        r"""Allows to get streams_container output of the operator

        Streams created from the input data sources, including aggregated streams across namespaces and any upstream streams.

        Returns
        -------
        output:
            An Output instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.metadata.streams_provider()
        >>> # Get the output from op.outputs. ...
        >>> result_streams_container = op.outputs.streams_container()
        """
        return self._streams_container
