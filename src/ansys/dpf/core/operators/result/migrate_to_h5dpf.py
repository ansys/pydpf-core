"""
migrate_to_h5dpf

Autogenerated DPF operator classes.
"""

from __future__ import annotations

from warnings import warn
from ansys.dpf.core.dpf_operator import Operator
from ansys.dpf.core.inputs import Input, _Inputs
from ansys.dpf.core.outputs import Output, _Outputs
from ansys.dpf.core.operators.specification import PinSpecification, Specification
from ansys.dpf.core.config import Config
from ansys.dpf.core.server_types import AnyServerType


class migrate_to_h5dpf(Operator):
    r"""Read mesh properties from the results files contained in the streams or
    data sources and make those properties available through a mesh
    selection manager in output.


    Parameters
    ----------
    dataset_size_compression_threshold: int, optional
        Integer value that defines the minimum dataset size (in bytes) to use h5 native compression Applicable for arrays of floats, doubles and integers.
    h5_native_compression: int or DataTree, optional
        Integer value / DataTree that defines the h5 native compression used For Integer Input {0: No Compression (default); 1-9: GZIP Compression : 9 provides maximum compression but at the slowest speed.}For DataTree Input {type: None / GZIP / ZSTD; level: GZIP (1-9) / ZSTD (1-20); num_threads: ZSTD (>0)}
    export_floats: bool, optional
        Converts double to float to reduce file size (default is true).If False, nodal results are exported as double precision and elemental results as single precision.
    filename: str
        filename of the migrated file
    comma_separated_list_of_results: str, optional
        list of results (source operator names) separated by semicolons that will be stored. (Example: U;S;EPEL). If empty, all available results will be converted.
    all_time_sets: bool, optional
        default is false
    streams_container: StreamsContainer, optional
        streams (result file container) (optional)
    data_sources: DataSources, optional
        if the stream is null then we need to get the file path from the data sources
    compression_workflow: Workflow or GenericDataContainer, optional
        BETA Option: Applies input compression workflow. User can input a GenericDataContainer that will map a compression workflow to a result name. Example of Map: {{ default: wf1}, {EUL: wf2}, {ENG_SE: wf3}}
    filtering_workflow: Workflow or GenericDataContainer, optional
        Applies input filtering workflow. User can input a GenericDataContainer of the format described for Pin(6) that will map a filtering workflow to a result name.

    Returns
    -------
    migrated_file: DataSources

    Examples
    --------
    >>> from ansys.dpf import core as dpf

    >>> # Instantiate operator
    >>> op = dpf.operators.result.migrate_to_h5dpf()

    >>> # Make input connections
    >>> my_dataset_size_compression_threshold = int()
    >>> op.inputs.dataset_size_compression_threshold.connect(my_dataset_size_compression_threshold)
    >>> my_h5_native_compression = int()
    >>> op.inputs.h5_native_compression.connect(my_h5_native_compression)
    >>> my_export_floats = bool()
    >>> op.inputs.export_floats.connect(my_export_floats)
    >>> my_filename = str()
    >>> op.inputs.filename.connect(my_filename)
    >>> my_comma_separated_list_of_results = str()
    >>> op.inputs.comma_separated_list_of_results.connect(my_comma_separated_list_of_results)
    >>> my_all_time_sets = bool()
    >>> op.inputs.all_time_sets.connect(my_all_time_sets)
    >>> my_streams_container = dpf.StreamsContainer()
    >>> op.inputs.streams_container.connect(my_streams_container)
    >>> my_data_sources = dpf.DataSources()
    >>> op.inputs.data_sources.connect(my_data_sources)
    >>> my_compression_workflow = dpf.Workflow()
    >>> op.inputs.compression_workflow.connect(my_compression_workflow)
    >>> my_filtering_workflow = dpf.Workflow()
    >>> op.inputs.filtering_workflow.connect(my_filtering_workflow)

    >>> # Instantiate operator and connect inputs in one line
    >>> op = dpf.operators.result.migrate_to_h5dpf(
    ...     dataset_size_compression_threshold=my_dataset_size_compression_threshold,
    ...     h5_native_compression=my_h5_native_compression,
    ...     export_floats=my_export_floats,
    ...     filename=my_filename,
    ...     comma_separated_list_of_results=my_comma_separated_list_of_results,
    ...     all_time_sets=my_all_time_sets,
    ...     streams_container=my_streams_container,
    ...     data_sources=my_data_sources,
    ...     compression_workflow=my_compression_workflow,
    ...     filtering_workflow=my_filtering_workflow,
    ... )

    >>> # Get output data
    >>> result_migrated_file = op.outputs.migrated_file()
    """

    def __init__(
        self,
        dataset_size_compression_threshold=None,
        h5_native_compression=None,
        export_floats=None,
        filename=None,
        comma_separated_list_of_results=None,
        all_time_sets=None,
        streams_container=None,
        data_sources=None,
        compression_workflow=None,
        filtering_workflow=None,
        config=None,
        server=None,
    ):
        super().__init__(name="hdf5::h5dpf::migrate_file", config=config, server=server)
        self._inputs = InputsMigrateToH5Dpf(self)
        self._outputs = OutputsMigrateToH5Dpf(self)
        if dataset_size_compression_threshold is not None:
            self.inputs.dataset_size_compression_threshold.connect(
                dataset_size_compression_threshold
            )
        if h5_native_compression is not None:
            self.inputs.h5_native_compression.connect(h5_native_compression)
        if export_floats is not None:
            self.inputs.export_floats.connect(export_floats)
        if filename is not None:
            self.inputs.filename.connect(filename)
        if comma_separated_list_of_results is not None:
            self.inputs.comma_separated_list_of_results.connect(
                comma_separated_list_of_results
            )
        if all_time_sets is not None:
            self.inputs.all_time_sets.connect(all_time_sets)
        if streams_container is not None:
            self.inputs.streams_container.connect(streams_container)
        if data_sources is not None:
            self.inputs.data_sources.connect(data_sources)
        if compression_workflow is not None:
            self.inputs.compression_workflow.connect(compression_workflow)
        if filtering_workflow is not None:
            self.inputs.filtering_workflow.connect(filtering_workflow)

    @staticmethod
    def _spec() -> Specification:
        description = r"""Read mesh properties from the results files contained in the streams or
data sources and make those properties available through a mesh
selection manager in output.
"""
        spec = Specification(
            description=description,
            map_input_pin_spec={
                -5: PinSpecification(
                    name="dataset_size_compression_threshold",
                    type_names=["int32"],
                    optional=True,
                    document=r"""Integer value that defines the minimum dataset size (in bytes) to use h5 native compression Applicable for arrays of floats, doubles and integers.""",
                ),
                -2: PinSpecification(
                    name="h5_native_compression",
                    type_names=["int32", "abstract_data_tree"],
                    optional=True,
                    document=r"""Integer value / DataTree that defines the h5 native compression used For Integer Input {0: No Compression (default); 1-9: GZIP Compression : 9 provides maximum compression but at the slowest speed.}For DataTree Input {type: None / GZIP / ZSTD; level: GZIP (1-9) / ZSTD (1-20); num_threads: ZSTD (>0)}""",
                ),
                -1: PinSpecification(
                    name="export_floats",
                    type_names=["bool"],
                    optional=True,
                    document=r"""Converts double to float to reduce file size (default is true).If False, nodal results are exported as double precision and elemental results as single precision.""",
                ),
                0: PinSpecification(
                    name="filename",
                    type_names=["string"],
                    optional=False,
                    document=r"""filename of the migrated file""",
                ),
                1: PinSpecification(
                    name="comma_separated_list_of_results",
                    type_names=["string"],
                    optional=True,
                    document=r"""list of results (source operator names) separated by semicolons that will be stored. (Example: U;S;EPEL). If empty, all available results will be converted.  """,
                ),
                2: PinSpecification(
                    name="all_time_sets",
                    type_names=["bool"],
                    optional=True,
                    document=r"""default is false""",
                ),
                3: PinSpecification(
                    name="streams_container",
                    type_names=["streams_container"],
                    optional=True,
                    document=r"""streams (result file container) (optional)""",
                ),
                4: PinSpecification(
                    name="data_sources",
                    type_names=["data_sources"],
                    optional=True,
                    document=r"""if the stream is null then we need to get the file path from the data sources""",
                ),
                6: PinSpecification(
                    name="compression_workflow",
                    type_names=["workflow", "generic_data_container"],
                    optional=True,
                    document=r"""BETA Option: Applies input compression workflow. User can input a GenericDataContainer that will map a compression workflow to a result name. Example of Map: {{ default: wf1}, {EUL: wf2}, {ENG_SE: wf3}}""",
                ),
                7: PinSpecification(
                    name="filtering_workflow",
                    type_names=["workflow", "generic_data_container"],
                    optional=True,
                    document=r"""Applies input filtering workflow. User can input a GenericDataContainer of the format described for Pin(6) that will map a filtering workflow to a result name.""",
                ),
            },
            map_output_pin_spec={
                0: PinSpecification(
                    name="migrated_file",
                    type_names=["data_sources"],
                    optional=False,
                    document=r"""""",
                ),
            },
        )
        return spec

    @staticmethod
    def default_config(server: AnyServerType = None) -> Config:
        """Returns the default config of the operator.

        This config can then be changed to the user needs and be used to
        instantiate the operator. The Configuration allows to customize
        how the operation will be processed by the operator.

        Parameters
        ----------
        server:
            Server with channel connected to the remote or local instance. When
            ``None``, attempts to use the global server.

        Returns
        -------
        config:
            A new Config instance equivalent to the default config for this operator.
        """
        return Operator.default_config(name="hdf5::h5dpf::migrate_file", server=server)

    @property
    def inputs(self) -> InputsMigrateToH5Dpf:
        """Enables to connect inputs to the operator

        Returns
        --------
        inputs:
            An instance of InputsMigrateToH5Dpf.
        """
        return super().inputs

    @property
    def outputs(self) -> OutputsMigrateToH5Dpf:
        """Enables to get outputs of the operator by evaluating it

        Returns
        --------
        outputs:
            An instance of OutputsMigrateToH5Dpf.
        """
        return super().outputs


class InputsMigrateToH5Dpf(_Inputs):
    """Intermediate class used to connect user inputs to
    migrate_to_h5dpf operator.

    Examples
    --------
    >>> from ansys.dpf import core as dpf
    >>> op = dpf.operators.result.migrate_to_h5dpf()
    >>> my_dataset_size_compression_threshold = int()
    >>> op.inputs.dataset_size_compression_threshold.connect(my_dataset_size_compression_threshold)
    >>> my_h5_native_compression = int()
    >>> op.inputs.h5_native_compression.connect(my_h5_native_compression)
    >>> my_export_floats = bool()
    >>> op.inputs.export_floats.connect(my_export_floats)
    >>> my_filename = str()
    >>> op.inputs.filename.connect(my_filename)
    >>> my_comma_separated_list_of_results = str()
    >>> op.inputs.comma_separated_list_of_results.connect(my_comma_separated_list_of_results)
    >>> my_all_time_sets = bool()
    >>> op.inputs.all_time_sets.connect(my_all_time_sets)
    >>> my_streams_container = dpf.StreamsContainer()
    >>> op.inputs.streams_container.connect(my_streams_container)
    >>> my_data_sources = dpf.DataSources()
    >>> op.inputs.data_sources.connect(my_data_sources)
    >>> my_compression_workflow = dpf.Workflow()
    >>> op.inputs.compression_workflow.connect(my_compression_workflow)
    >>> my_filtering_workflow = dpf.Workflow()
    >>> op.inputs.filtering_workflow.connect(my_filtering_workflow)
    """

    def __init__(self, op: Operator):
        super().__init__(migrate_to_h5dpf._spec().inputs, op)
        self._dataset_size_compression_threshold = Input(
            migrate_to_h5dpf._spec().input_pin(-5), -5, op, -1
        )
        self._inputs.append(self._dataset_size_compression_threshold)
        self._h5_native_compression = Input(
            migrate_to_h5dpf._spec().input_pin(-2), -2, op, -1
        )
        self._inputs.append(self._h5_native_compression)
        self._export_floats = Input(migrate_to_h5dpf._spec().input_pin(-1), -1, op, -1)
        self._inputs.append(self._export_floats)
        self._filename = Input(migrate_to_h5dpf._spec().input_pin(0), 0, op, -1)
        self._inputs.append(self._filename)
        self._comma_separated_list_of_results = Input(
            migrate_to_h5dpf._spec().input_pin(1), 1, op, -1
        )
        self._inputs.append(self._comma_separated_list_of_results)
        self._all_time_sets = Input(migrate_to_h5dpf._spec().input_pin(2), 2, op, -1)
        self._inputs.append(self._all_time_sets)
        self._streams_container = Input(
            migrate_to_h5dpf._spec().input_pin(3), 3, op, -1
        )
        self._inputs.append(self._streams_container)
        self._data_sources = Input(migrate_to_h5dpf._spec().input_pin(4), 4, op, -1)
        self._inputs.append(self._data_sources)
        self._compression_workflow = Input(
            migrate_to_h5dpf._spec().input_pin(6), 6, op, -1
        )
        self._inputs.append(self._compression_workflow)
        self._filtering_workflow = Input(
            migrate_to_h5dpf._spec().input_pin(7), 7, op, -1
        )
        self._inputs.append(self._filtering_workflow)

    @property
    def dataset_size_compression_threshold(self) -> Input:
        r"""Allows to connect dataset_size_compression_threshold input to the operator.

        Integer value that defines the minimum dataset size (in bytes) to use h5 native compression Applicable for arrays of floats, doubles and integers.

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.dataset_size_compression_threshold.connect(my_dataset_size_compression_threshold)
        >>> # or
        >>> op.inputs.dataset_size_compression_threshold(my_dataset_size_compression_threshold)
        """
        return self._dataset_size_compression_threshold

    @property
    def h5_native_compression(self) -> Input:
        r"""Allows to connect h5_native_compression input to the operator.

        Integer value / DataTree that defines the h5 native compression used For Integer Input {0: No Compression (default); 1-9: GZIP Compression : 9 provides maximum compression but at the slowest speed.}For DataTree Input {type: None / GZIP / ZSTD; level: GZIP (1-9) / ZSTD (1-20); num_threads: ZSTD (>0)}

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.h5_native_compression.connect(my_h5_native_compression)
        >>> # or
        >>> op.inputs.h5_native_compression(my_h5_native_compression)
        """
        return self._h5_native_compression

    @property
    def export_floats(self) -> Input:
        r"""Allows to connect export_floats input to the operator.

        Converts double to float to reduce file size (default is true).If False, nodal results are exported as double precision and elemental results as single precision.

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.export_floats.connect(my_export_floats)
        >>> # or
        >>> op.inputs.export_floats(my_export_floats)
        """
        return self._export_floats

    @property
    def filename(self) -> Input:
        r"""Allows to connect filename input to the operator.

        filename of the migrated file

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.filename.connect(my_filename)
        >>> # or
        >>> op.inputs.filename(my_filename)
        """
        return self._filename

    @property
    def comma_separated_list_of_results(self) -> Input:
        r"""Allows to connect comma_separated_list_of_results input to the operator.

        list of results (source operator names) separated by semicolons that will be stored. (Example: U;S;EPEL). If empty, all available results will be converted.

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.comma_separated_list_of_results.connect(my_comma_separated_list_of_results)
        >>> # or
        >>> op.inputs.comma_separated_list_of_results(my_comma_separated_list_of_results)
        """
        return self._comma_separated_list_of_results

    @property
    def all_time_sets(self) -> Input:
        r"""Allows to connect all_time_sets input to the operator.

        default is false

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.all_time_sets.connect(my_all_time_sets)
        >>> # or
        >>> op.inputs.all_time_sets(my_all_time_sets)
        """
        return self._all_time_sets

    @property
    def streams_container(self) -> Input:
        r"""Allows to connect streams_container input to the operator.

        streams (result file container) (optional)

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.streams_container.connect(my_streams_container)
        >>> # or
        >>> op.inputs.streams_container(my_streams_container)
        """
        return self._streams_container

    @property
    def data_sources(self) -> Input:
        r"""Allows to connect data_sources input to the operator.

        if the stream is null then we need to get the file path from the data sources

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.data_sources.connect(my_data_sources)
        >>> # or
        >>> op.inputs.data_sources(my_data_sources)
        """
        return self._data_sources

    @property
    def compression_workflow(self) -> Input:
        r"""Allows to connect compression_workflow input to the operator.

        BETA Option: Applies input compression workflow. User can input a GenericDataContainer that will map a compression workflow to a result name. Example of Map: {{ default: wf1}, {EUL: wf2}, {ENG_SE: wf3}}

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.compression_workflow.connect(my_compression_workflow)
        >>> # or
        >>> op.inputs.compression_workflow(my_compression_workflow)
        """
        return self._compression_workflow

    @property
    def filtering_workflow(self) -> Input:
        r"""Allows to connect filtering_workflow input to the operator.

        Applies input filtering workflow. User can input a GenericDataContainer of the format described for Pin(6) that will map a filtering workflow to a result name.

        Returns
        -------
        input:
            An Input instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> op.inputs.filtering_workflow.connect(my_filtering_workflow)
        >>> # or
        >>> op.inputs.filtering_workflow(my_filtering_workflow)
        """
        return self._filtering_workflow


class OutputsMigrateToH5Dpf(_Outputs):
    """Intermediate class used to get outputs from
    migrate_to_h5dpf operator.

    Examples
    --------
    >>> from ansys.dpf import core as dpf
    >>> op = dpf.operators.result.migrate_to_h5dpf()
    >>> # Connect inputs : op.inputs. ...
    >>> result_migrated_file = op.outputs.migrated_file()
    """

    def __init__(self, op: Operator):
        super().__init__(migrate_to_h5dpf._spec().outputs, op)
        self._migrated_file = Output(migrate_to_h5dpf._spec().output_pin(0), 0, op)
        self._outputs.append(self._migrated_file)

    @property
    def migrated_file(self) -> Output:
        r"""Allows to get migrated_file output of the operator

        Returns
        -------
        output:
            An Output instance for this pin.

        Examples
        --------
        >>> from ansys.dpf import core as dpf
        >>> op = dpf.operators.result.migrate_to_h5dpf()
        >>> # Get the output from op.outputs. ...
        >>> result_migrated_file = op.outputs.migrated_file()
        """
        return self._migrated_file
