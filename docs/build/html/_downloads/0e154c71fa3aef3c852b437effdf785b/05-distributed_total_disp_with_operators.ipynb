{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distributed post without client connection to remote processes with Operators {#ref_distributed_total_disp_op}\r\n\r\nThis example shows how distributed files can be read and post processed\r\non distributed processes. After remote post processing, results a merged\r\non the local process.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import dpf module and its examples files\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from ansys.dpf import core as dpf\nfrom ansys.dpf.core import examples\nfrom ansys.dpf.core import operators as ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the template workflow of total displacement\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure the servers\r\n\r\nMake a list of ip addresses an port numbers on which dpf servers are\r\nstarted. Workflows instances will be created on each of those servers to\r\naddress each a different result file. In this example, we will post\r\nprocess an analysis distributed in 2 files, we will consequently require\r\n2 remote processes To make this example easier, we will start local\r\nservers here, but we could get connected to any existing servers on the\r\nnetwork.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "remote_servers = [dpf.start_local_server(as_global=False), dpf.start_local_server(as_global=False)]\nips = [remote_server.ip for remote_server in remote_servers]\nports = [remote_server.port for remote_server in remote_servers]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the ips and ports\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"ips:\", ips)\nprint(\"ports:\", ports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we show how we could send files in temporary directory if we were\r\nnot in shared memory\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "files = examples.download_distributed_files()\nserver_file_paths = [dpf.upload_file_in_tmp_folder(files[0], server=remote_servers[0]),\n                     dpf.upload_file_in_tmp_folder(files[1], server=remote_servers[1])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Send workflows on servers\r\n\r\nHere we create new instances on the server by copies of the template\r\nworkflow We also connect the data sources to those workflows\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "remote_operators = []\nfor i, server in enumerate(remote_servers):\n    displacement = ops.result.displacement(server=server)\n    norm = ops.math.norm_fc(displacement, server=server)\n    remote_operators.append(norm)\n    ds = dpf.DataSources(server_file_paths[i], server=server)\n    displacement.inputs.data_sources(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create a local workflow able to merge the results\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "merge = ops.utility.merge_fields_containers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connect the workflows together and get the output\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, server in enumerate(remote_servers):\n    merge.connect(i, remote_operators[i], 0)\n\nfc = merge.get_output(0, dpf.types.fields_container)\nprint(fc)\nprint(fc[0].min().data)\nprint(fc[0].max().data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}